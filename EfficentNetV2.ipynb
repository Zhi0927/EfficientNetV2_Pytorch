{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the width of the code block(Can ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\")) #<--- -----Change the width value[0-100] to what you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# EfficentNetV2: Smaller Models and Faster Training\n",
    "[(Tan, & Le,  2021)](https://arxiv.org/pdf/2104.00298.pdf)\n",
    "<img src=\"Assets/m4.png\" width=\"70%\"/>\n",
    "\n",
    "## Abstract\n",
    "In EfficientNetV1, the author focus on accuracy, parameters efficiency and FLOPs, but FLOPs are only indirect indicators of inference speed and cannot be used as speed evaluation criteria([Ma, Zhang, Zheng,& Sun, 2018](https://arxiv.org/abs/1807.11164)).\n",
    "1. **FLOPs do not consider important factors that may affect speed**\n",
    " * **MAC (memory access cost):** such as Group Conv, it will take up a lot of running time.\n",
    " * **Degree of parallelism:** Represents the number of repeated stacking of Operators in the current Stage. the measurement of how many operations a computer can perform at the same time. In the same FLOPs, models with high parallelism may be faster than models with low parallelism.\n",
    "2. **Run on different hardware:** In the same FLOPs, different hardware platforms will have different speeds.\n",
    "\n",
    "In EfficientNetV2, the author aim to imporve the training speed while maintaining the parameters efficiency. It can be seen from the table that V2 has greater advantages in training speed and inference speed compared to V1.\n",
    "\n",
    "<img src=\"Assets/m7.png\" width=\"40%\"/>\n",
    "\n",
    "### EfficientNetV2 is worth paying attention to\n",
    "In the past research, the main focus was on accuracy and parameter performance. In recent years, the improvement in accuracy has reached saturation, and attention has begun to focus on the training speed and inference speed of the network.\n",
    "1. Introduce the Fused-MBConv module\n",
    "2. Introduce a progressive learning strategy (training faster)\n",
    "\n",
    "## EfficientNetV1 vs EfficientNetV2\n",
    "### 1. The training speed is very slow when the size of the training image is large.\n",
    "Reduce the size of the training image to speed up the training and use a larger batch_size.\n",
    "\n",
    "<img src=\"Assets/m8.png\" width=\"60%\"/>\n",
    "\n",
    "### 2. Using Depthwise convolutions in the shallow layer of the network will be very slow.\n",
    "\n",
    "Using **Depthwise convolutions** in shallow networks can be very slow. It is not possible to make the most of  the existing accelerators (In theory, FLOPs are small, but they are not as fast as expected in fact), so the author introduced **Fused-MBConv**.\n",
    "\n",
    "<img src=\"Assets/m9.png\" width=\"60%\"/>\n",
    "\n",
    "### 3. Scaling up each stage equally is sub-optimal.\n",
    "In EfficientNetV1, the depth and width of each stage are scaling up equally. However, each stage has different effects on the training speed and parameters of the network, so the strategy of directly using the same scaling factor is unreasonable. So the author uses a non-uniform scaling factor strategy to scale the model.\n",
    "\n",
    "<img src=\"Assets/m10.png\" width=\"50%\"/>\n",
    "\n",
    "##### EfficientNetV1[(Tan, Le, 2019)](https://arxiv.org/pdf/1905.11946.pdf)\n",
    "1. **input_size** represents the image size of the input network when training the network.\n",
    "2. **width_coefficient** represents the scaling up factor in the channel.\n",
    "3. **depth_coefficient** represents the scaling up factor in the depth.\n",
    "4. **drop_connect_rate** is the drop_rate used by the dropout layer (Stochastic Depth) in the MBConv (increasing from 0 to drop_connect_rate).\n",
    "5. **dropout_rate** is the dropout_rate of dropout layer before the last FC layer.\n",
    "\n",
    "## Result\n",
    "1. The New Neural NetWork - EfficentV2, performs better than the previous network in terms of training and parameters.\n",
    "2. A method to improve progressive learning is proposed, which dynamically adjusts the regularization method（Dropout、Rand Augment、Mixup） according to the size of the image to improve the training speed and accuracy.\n",
    "3. The training speed is increased by 11 times (EfficientNet V2-M is compared with EfficientNet-B7), and the amount of parameters is reduced to 1/6.8.\n",
    "\n",
    "<img src=\"Assets/m11.png\" width=\"50%\"/>\n",
    "\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Implement\n",
    "\n",
    "<h3>Note: The use of Swish must torch>1.7 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch version: 1.8.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"Pytorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libarary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "import math\n",
    "\n",
    "from collections import OrderedDict\n",
    "from functools import partial\n",
    "from typing import Callable, Optional\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import Tensor\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Networks with Stochastic Depth\n",
    "\n",
    "<img src=\"Assets/m2.png\" width=\"50%\" style=\"float: left;\"/>\n",
    "\n",
    "##### Deep Networks with Stochastic Depth[(Huang, Sun, Liu, Sedra,& Weinberger, 2019)](https://arxiv.org/pdf/1603.09382.pdf)\n",
    "<p>\n",
    "1. Improve training speed.<br>\n",
    "2. Small increase in accuracy. <br>\n",
    "3. In EfficientNetV2, drop_prob ranges from 0 to 0.2(red block).<br>\n",
    "4. Applied to the layer called <b>DropOut</b> in Fused-MBConv and MBConv.\n",
    "</p>\n",
    "\n",
    "Drop paths (Stochastic Depth) per sample when applied in main path of residual blocks.\n",
    "This function is taken from the [rwightman](https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/layers/drop.py#L140)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#================================================= * Define DropPath Function * =================================================#\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype = x.dtype, device = x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "#=================================================== * Define Model Class * =====================================================#\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    def __init__(self, drop_prob = None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #self.training: if set model.eval(), self.training = False\n",
    "        return drop_path(x, self.drop_prob, self.training)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squeeze-and-Excitation(SE)\n",
    "<img src=\"Assets/m15.png\" width=\"50%\" style= \"float:left\"/>\n",
    "\n",
    "##### Squeeze-and-Excitation Networks[(Hu, Shen, Albanie, Sun,& Wu, 2017)](https://arxiv.org/pdf/1709.01507.pdf)\n",
    "<p>\n",
    "1. Average Pooling<br>\n",
    "2. The input channels of FC1 is <b>1/4 of the channels input to the MBConv</b>, and the <b>Swish</b> activation function is used.<br>\n",
    "3. The input channels of FC2 is the <b>channels output of the Depthwise Conv(SE Input)</b>, and the <b>Sigmoid</b> activation function is used.\n",
    "</p>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeExcite(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_channels: int,   # <------- MBConv Input channels.\n",
    "                 expand_channels: int,  # <------- SE Inputchannels (DW Output channels).\n",
    "                 se_ratio: float = 0.25):        \n",
    "        super(SqueezeExcite, self).__init__()\n",
    "        \n",
    "        \n",
    "        squeeze_channels = int(input_channels * se_ratio)\n",
    "        \n",
    "        #=============================================== * FC1 * =====================================================#\n",
    "        self.conv_reduce = nn.Conv2d(expand_channels, squeeze_channels, 1)\n",
    "        self.act1 = nn.SiLU()  # alias Swish\n",
    "        \n",
    "        #=============================================== * FC2 * =====================================================#\n",
    "        self.conv_expand = nn.Conv2d(squeeze_channels, expand_channels, 1)\n",
    "        self.act2 = nn.Sigmoid()\n",
    "\n",
    "        \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        scale = x.mean((2, 3), keepdim=True) # Global polling\n",
    "        scale = self.conv_reduce(scale)      # FC1\n",
    "        scale = self.act1(scale)             # Swish\n",
    "        scale = self.conv_expand(scale)      # FC2\n",
    "        scale = self.act2(scale)             # Sigmoid\n",
    "        return scale * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Conv + BN + Act  Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBNAct(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: int = 3,\n",
    "                 stride: int = 1,\n",
    "                 groups: int = 1,\n",
    "                 norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "                 activation_layer: Optional[Callable[..., nn.Module]] = None):        \n",
    "        super(ConvBNAct, self).__init__()\n",
    "\n",
    "        padding = (kernel_size - 1) // 2\n",
    "        \n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if activation_layer is None:\n",
    "            activation_layer = nn.SiLU\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels  = in_channels,\n",
    "                              out_channels = out_channels,\n",
    "                              kernel_size  = kernel_size,\n",
    "                              stride       = stride,\n",
    "                              padding      = padding,\n",
    "                              groups       = groups,\n",
    "                              bias         = False)\n",
    "\n",
    "        self.bn = norm_layer(out_planes)\n",
    "        self.act = activation_layer()\n",
    "\n",
    "    def forward(self, x):\n",
    "        result = self.conv(x)\n",
    "        result = self.bn(result)\n",
    "        result = self.act(result)\n",
    "\n",
    "        return result                                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MBConv Block\n",
    "<img src=\"Assets/m14.png\" width=\"60%\"/>\n",
    "\n",
    "##### EfficientNetV1[(Tan, Le, 2019)](https://arxiv.org/pdf/1905.11946.pdf)\n",
    "MBConv is actually an improved version of **InvertedResidualBlock** in the MobileNetV3[(Howard, Sandler, Chu, Chen, Chen, Tan,...& Adam, 2019)](https://arxiv.org/pdf/1905.02244.pdf). The difference is that the Swish activation function is used in MBConv, and the SE (Squeeze-and-Excitation) module is added to each MBConv.\n",
    "\n",
    "1. The first 1x1 convolutional layer used to increase the dimension, the number of convolution kernels is n times the input channel, n ∈ {1, 6}. **When n = 1, remove the first 1x1 convolutional layer** that used to increase dimensions, that is, the MBConv in Stage2 does not have the first 1x1 convolutional layer (similar to MobileNetV3).\n",
    "2. DepwiseConv kxk, where k is the kernel size, there are 3x3 and 5x5 in EfficentV1.\n",
    "3. **The shortcut only exists when the input and the output channel of MBConv have the same shape and stride ==1.**\n",
    "4. SE (Squeeze-and-Excitation)\n",
    "5. Conv1x1 is used to reduce dimensions.\n",
    "6. **Dropout(Stochastic Depth) only exists when using shortcut.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 kernel_size: int,\n",
    "                 input_channels: int,\n",
    "                 out_channels: int,\n",
    "                 expand_ratio: int,\n",
    "                 stride: int,\n",
    "                 se_ratio: float,\n",
    "                 drop_rate: float,\n",
    "                 norm_layer: Callable[..., nn.Module]):\n",
    "        super(MBConv, self).__init__()\n",
    "       \n",
    "        \n",
    "        if stride not in [1, 2]:\n",
    "            raise ValueError(\"illegal stride value.\")\n",
    "            \n",
    "        self.out_channels = out_channels       \n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "        #shortcut used condition.(stride == 1, in-channels == out-channels)\n",
    "        self.has_shortcut = (stride == 1 and input_channels == out_channels)\n",
    "\n",
    "        # alias Swish\n",
    "        activation_layer = nn.SiLU  \n",
    "        \n",
    "        # In EfficientNetV2, there is no expansion=1 in MBConv, so conv_pw must exist.\n",
    "        assert expand_ratio != 1\n",
    "        \n",
    "        #expand width\n",
    "        expanded_channels = input_channels * expand_ratio\n",
    "        \n",
    "           \n",
    "        #====================================================== * Point-wise Expansion(1x1, s1) * ==================================================#\n",
    "        # in:*para-IN* || out:*para-IN* x ratio || kernel:1 || stride: 1 || group: 1 || Nomormal: *para*(BN) || Act: SiLu\n",
    "        \n",
    "        self.expand_conv = ConvBNAct(in_channels      = input_channels,\n",
    "                                     out_channels     = expanded_channels,\n",
    "                                     kernel_size      = 1,\n",
    "                                     norm_layer       = norm_layer,\n",
    "                                     activation_layer = activation_layer)\n",
    "      \n",
    "        #==================================================== * Depth-wise convolution(3x3, s1/s2) * ===============================================#\n",
    "        # in:*para-IN* x ratio || out:*para-IN* x ratio || kernel: *para* || stride: *para* || group: *para-IN* x ratio || Nomormal: *para*(BN) || Act: SiLu\n",
    "        \n",
    "        self.dwconv = ConvBNAct(in_channels      = expanded_channels,\n",
    "                                out_channels     = expanded_channels,\n",
    "                                kernel_size      = kernel_size,\n",
    "                                stride           = stride,\n",
    "                                groups           = expanded_channels,\n",
    "                                norm_layer       = norm_layer,\n",
    "                                activation_layer = activation_layer)\n",
    "\n",
    "        #======================================================== * Squeeze-and-Excitation(SE) * ===================================================#\n",
    "        # input_channel:*para-IN*(MBConv-IN) || expanded_channel: *para-IN* x ratio(DW-OUT)\n",
    "        \n",
    "        self.SE = SqueezeExcite(input_channels   = input_channels,\n",
    "                                expand_channels  = expanded_channels,\n",
    "                                se_ratio         = se_ratio) if se_ratio > 0 else nn.Identity()\n",
    "        \n",
    "        #======================================================= * Point-wise linear projection * ==================================================#\n",
    "        # in:*para-IN* x ratio || out: *para-OUT* || kernel: 1 || stride: 1 || group: 1 || Nomormal: *para*(BN) || Act: None\n",
    "        \n",
    "        self.project_conv = ConvBNAct(in_channels       = expanded_channels,\n",
    "                                      out_channels      = out_channels,\n",
    "                                      kernel_size       = 1,\n",
    "                                      norm_layer        = norm_layer,\n",
    "                                      activation_layer  = nn.Identity)  # There is no activation function, so use Identity (empty layer)\n",
    "\n",
    "\n",
    "        #================================================================ * Drop Path * ============================================================#\n",
    "        # Use the dropout layer only when using the shortcut.\n",
    "        if self.has_shortcut and drop_rate > 0:\n",
    "            self.dropout = DropPath(drop_rate)\n",
    "\n",
    "            \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        result = self.expand_conv(x)\n",
    "        result = self.dwconv(result)\n",
    "        result = self.SE(result)\n",
    "        result = self.project_conv(result)\n",
    "\n",
    "        #shortcut used condition.(stride == 1, in-channels == out-channels)\n",
    "        if self.has_shortcut:\n",
    "            if self.drop_rate > 0:\n",
    "                result = self.dropout(result)\n",
    "            result += x\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fused-MBConv Block\n",
    "\n",
    "<img src=\"Assets/m13.png\" width=\"60%\"/>\n",
    "\n",
    "1. When **expansion =1**, On the main branch, there is **only 3×3 ProjectConv**, follow by BN, SILU(activation functions), and **Dropout**.\n",
    "2. When **expansion !=1**, On the main branch, there is an **3×3ExpandConv** to increase dimensions , followed by BN and SILU(activation functions), and then **1×1ProjectConv**, final BN and **Dropout**.\n",
    "3. The **shortcut** only exists when **stride=1 and the input channel is equal to the output channel** of the main branch.\n",
    "4. **Dropout(Stochastic Depth) only exists when the Shortcut is used.**\n",
    "5. **Fused-MBConv does not use Squeeze-and-Excitation(SE).**\n",
    "\n",
    "\n",
    " `Problems that occur when BN and Dropout are used together`[(Li, Chen, Hu, &Yang, 2018)](https://arxiv.org/pdf/1801.05134.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedMBConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 kernel_size: int,\n",
    "                 input_channels: int,\n",
    "                 out_channels: int,\n",
    "                 expand_ratio: int,\n",
    "                 stride: int,\n",
    "                 se_ratio: float,\n",
    "                 drop_rate: float,\n",
    "                 norm_layer: Callable[..., nn.Module]):\n",
    "        super(FusedMBConv, self).__init__()\n",
    "\n",
    "        \n",
    "        assert stride in [1, 2]\n",
    "        assert se_ratio == 0\n",
    "\n",
    "        self.out_channels = out_channels        \n",
    "        self.drop_rate = drop_rate\n",
    "        \n",
    "        # shortcut used condition.(stride == 1, in-channels == out-channels)\n",
    "        self.has_shortcut = (stride == 1 and input_channels == out_channels)\n",
    "\n",
    "        # alias Swish\n",
    "        activation_layer = nn.SiLU  \n",
    "        \n",
    "        #expand width\n",
    "        expanded_channels = input_channels * expand_ratio\n",
    "        \n",
    "        # Expand-Conv only exists when the expansion ratio !=1.\n",
    "        self.has_expansion = (expand_ratio != 1)\n",
    "        \n",
    "        \n",
    "        ################################################################### * Expansion != 1 * ###########################################################\n",
    "        if self.has_expansion:\n",
    "            \n",
    "            #===================================================== * Expansion convolution(3x3, s1/S2) * ===============================================#\n",
    "            # in:*para-IN* || out:*para-IN* x ratio || kernel: *para* || stride: *para* || group: 1 || Nomormal: *para*(BN) || Act: SiLu\n",
    "            self.expand_conv = ConvBNAct(in_channels      = input_channels,\n",
    "                                         out_channels     = expanded_channels,\n",
    "                                         kernel_size      = kernel_size,\n",
    "                                         stride           = stride,\n",
    "                                         norm_layer       = norm_layer,\n",
    "                                         activation_layer = activation_layer)\n",
    "            \n",
    "            #==================================================== * Point-wise linear projection(1x1, s1) * ============================================#\n",
    "            # in:*para-IN* x ratio || out:*para-OUT* || kernel: 1 || stride: 1 || group: 1 || Nomormal: *para*(BN) || Act: None\n",
    "            self.project_conv = ConvBNAct(in_channels      = expanded_channels,\n",
    "                                          out_channels     = out_channels,\n",
    "                                          kernel_size      = 1,\n",
    "                                          norm_layer       = norm_layer,\n",
    "                                          activation_layer = nn.Identity)  # There is no activation function, so use Identity (empty layer)\n",
    "            \n",
    "       \n",
    "        ################################################################### * Expansion = 1 * ############################################################\n",
    "        # When only project_conv exists.\n",
    "        else:\n",
    "            \n",
    "            #=================================================== * Point-wise linear projection(3x3, s1/s2) * ==========================================#\n",
    "            # in:*para-IN* || out:*para-OUT* || kernel: *para* || stride: *para* || group: 1 || Nomormal: *para*(BN) || Act: SiLu\n",
    "            self.project_conv = ConvBNAct(in_channels      = in_channels,\n",
    "                                          out_channels     = out_channels,\n",
    "                                          kernel_size      = kernel_size,\n",
    "                                          stride           = stride,\n",
    "                                          norm_layer       = norm_layer,\n",
    "                                          activation_layer = activation_layer)  # There is activation function.\n",
    "\n",
    "        \n",
    "        #=================================================================== * Drop Path * =============================================================#\n",
    "        # Use the dropout layer only when using the shortcut.\n",
    "        if self.has_shortcut and drop_rate > 0:\n",
    "            self.dropout = DropPath(drop_rate)\n",
    "\n",
    "            \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.has_expansion:\n",
    "            result = self.expand_conv(x)\n",
    "            result = self.project_conv(result)\n",
    "        else:\n",
    "            result = self.project_conv(x)\n",
    "\n",
    "        # shortcut used condition.(stride == 1, in-channels == out-channels)\n",
    "        if self.has_shortcut:\n",
    "            if self.drop_rate > 0:\n",
    "                result = self.dropout(result)\n",
    "            result += x\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Efficentv2 Base Model\n",
    "\n",
    "<img src=\"Assets/m12.png\" width=\"50%\" style=\"float: left;\"/>\n",
    "<p> \n",
    "<h5> Different with EfficentV1</h5>\n",
    "1. In addition to using MBConv, Fused-MBConv module is also used.<br>\n",
    "2. Use a smaller expansion ratio (In V1 is 6). <br>\n",
    "3. Prefer to use smaller kernel_size(3×3). In V1, used 5x5.<br>\n",
    "4. Removed the last stage with stride 1 in EfficientNetV1 (stage8 in V1_S)\n",
    "</p>\n",
    "<p>\n",
    "<h5>Where:</h5>\n",
    "1. <b>MBConv4</b> represents the expansion factor of the first convolutional layer on the main branch is 4.<br>\n",
    "2. <b>SE0.25</b> represents the channels in the first full connected layer of the SE module is 1/4 of the channels of the input MBConv.<br>\n",
    "3. <b>Layers</b> is the number of repeats.<br>\n",
    "4. <b>Stride</b> that value only applies to the first block, else is 1.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of each layer (model_cnf)\n",
    "\n",
    "1. First dimension: represents Stages **(Excluding Stage0 and Stage7)**.\n",
    "2. Second dimension:\n",
    "> * [0] represents the number of **repeated stacking of Operators** in the current Stage.\n",
    "  * [1] represents **kernel_size**.\n",
    "  * [2] represents **stride**.\n",
    "  * [3] represents **expansion ratio**.\n",
    "  * [4] represents **input channels**.\n",
    "  * [5] represents **output channels**.\n",
    "  * [6] represents conv_type, **0 is Fused-MBConv**, **1 is MBConv**.\n",
    "  * [7] represents using SE, and **se_ratio**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model_cnf: list,\n",
    "                 num_classes: int = 1000,\n",
    "                 num_features: int = 1280,\n",
    "                 dropout_rate: float = 0.2,\n",
    "                 drop_connect_rate: float = 0.2):\n",
    "        super(EfficientNetV2, self).__init__()\n",
    "\n",
    "        # The len of each layer of para must be 8.\n",
    "        for cnf in model_cnf:\n",
    "            assert len(cnf) == 8\n",
    "\n",
    "        # Bind BatchNorm param, eps = 10^-3, momentum = 0.1\n",
    "        norm_layer = partial(nn.BatchNorm2d, eps = 1e-3, momentum=0.1)\n",
    "        \n",
    "        \n",
    "        #========================================================= * Stage 0 * =========================================================#\n",
    "        # Stage0 Conv (In: 3(RGB Image) || Out: stage1-IN || kernel: 3 || stride: 2 || norm: BN || Act: SiLu)\n",
    "        \n",
    "        self.stem = ConvBNAct(in_channels = 3,\n",
    "                              out_channels = model_cnf[0][4],\n",
    "                              kernel_size = 3,\n",
    "                              stride = 2,\n",
    "                              norm_layer = norm_layer)  # Default Act is SiLu\n",
    "        \n",
    "        #======================================================== * Stage 1-6 * ========================================================#\n",
    "        total_blocks = sum([i[0] for i in model_cnf])\n",
    "        block_id = 0\n",
    "        blocks = []       \n",
    "        for cnf in model_cnf:\n",
    "            repeats = cnf[0]\n",
    "            Operator_FMBConv = FusedMBConv if cnf[-2] == 0 else MBConv\n",
    "            for i in range(repeats):\n",
    "                blocks.append(Operator_FMBConv( kernel_size    = cnf[1],\n",
    "                                                input_channels = cnf[4] if i == 0 else cnf[5],\n",
    "                                                out_channels   = cnf[5],\n",
    "                                                expand_ratio   = cnf[3],\n",
    "                                                stride         = cnf[2] if i == 0 else 1,\n",
    "                                                se_ratio       = cnf[-1],\n",
    "                                                drop_rate      = drop_connect_rate * block_id / total_blocks,\n",
    "                                                norm_layer     = norm_layer))\n",
    "                block_id += 1                \n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        \n",
    "        #========================================================= * Stage 7 * =========================================================#\n",
    "        \n",
    "        head = OrderedDict()\n",
    "\n",
    "        head.update({\"project_conv\": ConvBNAct(in_channels  = model_cnf[-1][-3],\n",
    "                                               out_channels = num_features,\n",
    "                                               kernel_size  = 1,\n",
    "                                               norm_layer   = norm_layer)})  # Default Act is SiLu\n",
    "\n",
    "        head.update({\"avgpool\": nn.AdaptiveAvgPool2d(1)})\n",
    "        \n",
    "        head.update({\"flatten\": nn.Flatten()})\n",
    "\n",
    "        if dropout_rate > 0:\n",
    "            head.update({\"dropout\": nn.Dropout(p = dropout_rate, inplace=True)})\n",
    "            \n",
    "        head.update({\"classifier\": nn.Linear(num_features, num_classes)})\n",
    "\n",
    "        self.head = nn.Sequential(head)\n",
    "        \n",
    "        #====================================================== * Initial Weights * =====================================================#\n",
    "        \n",
    "        for m in self.modules():\n",
    "            \n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode = \"fan_out\")\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "                    \n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "                \n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "                \n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        x = self.stem(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Define EfficientNetV2_S Model\n",
    "\n",
    "* Image_size:\n",
    "  * train_size: 300\n",
    "  * eval_size: 384\n",
    "* Model_config: repeat, kernel, stride, expansion, in_c, out_c, operator, se_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficientnetv2_s(num_classes: int = 1000):\n",
    "\n",
    "    \n",
    "    model_config = [[2, 3, 1, 1, 24, 24, 0, 0],\n",
    "                    [4, 3, 2, 4, 24, 48, 0, 0],\n",
    "                    [4, 3, 2, 4, 48, 64, 0, 0],\n",
    "                    [6, 3, 2, 4, 64, 128, 1, 0.25],\n",
    "                    [9, 3, 1, 6, 128, 160, 1, 0.25],\n",
    "                    [15, 3, 2, 6, 160, 256, 1, 0.25]]\n",
    "\n",
    "    model = EfficientNetV2(model_cnf    = model_config,\n",
    "                           num_classes  = num_classes,\n",
    "                           dropout_rate = 0.2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define EfficientNetV2_M Model\n",
    "\n",
    "* Image_size:\n",
    "  * train_size: 384\n",
    "  * eval_size: 480\n",
    "* Model_config: repeat, kernel, stride, expansion, in_c, out_c, operator, se_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficientnetv2_m(num_classes: int = 1000):\n",
    "    \n",
    "    model_config = [[3, 3, 1, 1, 24, 24, 0, 0],\n",
    "                    [5, 3, 2, 4, 24, 48, 0, 0],\n",
    "                    [5, 3, 2, 4, 48, 80, 0, 0],\n",
    "                    [7, 3, 2, 4, 80, 160, 1, 0.25],\n",
    "                    [14, 3, 1, 6, 160, 176, 1, 0.25],\n",
    "                    [18, 3, 2, 6, 176, 304, 1, 0.25],\n",
    "                    [5, 3, 1, 6, 304, 512, 1, 0.25]]\n",
    "\n",
    "    model = EfficientNetV2(model_cnf    = model_config,\n",
    "                           num_classes  = num_classes,\n",
    "                           dropout_rate = 0.3)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define EfficientNetV2_L Model\n",
    "\n",
    "* Image_size:\n",
    "  * train_size: 384\n",
    "  * eval_size: 480\n",
    "* Model_config: repeat, kernel, stride, expansion, in_c, out_c, operator, se_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficientnetv2_l(num_classes: int = 1000):\n",
    "    \n",
    "    model_config = [[4, 3, 1, 1, 32, 32, 0, 0],\n",
    "                    [7, 3, 2, 4, 32, 64, 0, 0],\n",
    "                    [7, 3, 2, 4, 64, 96, 0, 0],\n",
    "                    [10, 3, 2, 4, 96, 192, 1, 0.25],\n",
    "                    [19, 3, 1, 6, 192, 224, 1, 0.25],\n",
    "                    [25, 3, 2, 6, 224, 384, 1, 0.25],\n",
    "                    [7, 3, 1, 6, 384, 640, 1, 0.25]]\n",
    "\n",
    "    model = EfficientNetV2(model_cnf     = model_config,\n",
    "                           num_classes   = num_classes,\n",
    "                           dropout_rat   = 0.4)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Define DataSet torch class\n",
    "\n",
    "The official implementation of **default_collate** can refer to [this](https://github.com/pytorch/pytorch/blob/67b7e751e6b5931a9f45274653f4f653a4e6cdf6/torch/utils/data/_utils/collate.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self, images_path: list, images_class: list, transform=None):\n",
    "        self.images_path = images_path\n",
    "        self.images_class = images_class\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_path)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img = Image.open(self.images_path[item])\n",
    "        # RGB is a color image, L is a grayscale image.\n",
    "        if img.mode != 'RGB':\n",
    "            raise ValueError(\"image: {} isn't RGB mode.\".format(self.images_path[item]))\n",
    "            \n",
    "        label = self.images_class[item]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "    \n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        images, labels = tuple(zip(*batch))\n",
    "\n",
    "        images = torch.stack(images, dim=0)\n",
    "        labels = torch.as_tensor(labels)\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Preprocess Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_split_data(root: str, val_rate: float = 0.2):\n",
    "    \n",
    "    # Fixed random value.\n",
    "    random.seed(0) \n",
    "    assert os.path.exists(root), \"dataset root: {} does not exist.\".format(root)\n",
    "\n",
    "    # 遍历文件夹，一个文件夹对应一个类别\n",
    "    flower_class = [cla for cla in os.listdir(root) if os.path.isdir(os.path.join(root, cla))]\n",
    "    \n",
    "    # 排序，保证顺序一致\n",
    "    flower_class.sort()\n",
    "    \n",
    "    # 生成类别名称以及对应的数字索引\n",
    "    class_indices = dict((k, v) for v, k in enumerate(flower_class))\n",
    "    json_str = json.dumps(dict((val, key) for key, val in class_indices.items()), indent=4)\n",
    "    with open('class_indices.json', 'w') as json_file:\n",
    "        json_file.write(json_str)\n",
    "\n",
    "    train_images_path = []  # 存储训练集的所有图片路径\n",
    "    train_images_label = []  # 存储训练集图片对应索引信息\n",
    "    val_images_path = []  # 存储验证集的所有图片路径\n",
    "    val_images_label = []  # 存储验证集图片对应索引信息\n",
    "    every_class_num = []  # 存储每个类别的样本总数\n",
    "    \n",
    "    supported = [\".jpg\", \".JPG\", \".png\", \".PNG\"]  # 支持的文件后缀类型\n",
    "    # 遍历每个文件夹下的文件\n",
    "    for cla in flower_class:\n",
    "        cla_path = os.path.join(root, cla)\n",
    "        # 遍历获取supported支持的所有文件路径\n",
    "        images = [os.path.join(root, cla, i) for i in os.listdir(cla_path)\n",
    "                  if os.path.splitext(i)[-1] in supported]\n",
    "        # 获取该类别对应的索引\n",
    "        image_class = class_indices[cla]\n",
    "        # 记录该类别的样本数量\n",
    "        every_class_num.append(len(images))\n",
    "        # 按比例随机采样验证样本\n",
    "        val_path = random.sample(images, k=int(len(images) * val_rate))\n",
    "\n",
    "        for img_path in images:\n",
    "            if img_path in val_path:  # 如果该路径在采样的验证集样本中则存入验证集\n",
    "                val_images_path.append(img_path)\n",
    "                val_images_label.append(image_class)\n",
    "            else:  # 否则存入训练集\n",
    "                train_images_path.append(img_path)\n",
    "                train_images_label.append(image_class)\n",
    "\n",
    "    print(\"{} images were found in the dataset.\".format(sum(every_class_num)))\n",
    "    print(\"{} images for training.\".format(len(train_images_path)))\n",
    "    print(\"{} images for validation.\".format(len(val_images_path)))\n",
    "\n",
    "    plot_image = False\n",
    "    if plot_image:\n",
    "        # 绘制每种类别个数柱状图\n",
    "        plt.bar(range(len(flower_class)), every_class_num, align='center')\n",
    "        # 将横坐标0,1,2,3,4替换为相应的类别名称\n",
    "        plt.xticks(range(len(flower_class)), flower_class)\n",
    "        # 在柱状图上添加数值标签\n",
    "        for i, v in enumerate(every_class_num):\n",
    "            plt.text(x=i, y=v + 5, s=str(v), ha='center')\n",
    "        # 设置x坐标\n",
    "        plt.xlabel('image class')\n",
    "        # 设置y坐标\n",
    "        plt.ylabel('number of images')\n",
    "        # 设置柱状图的标题\n",
    "        plt.title('flower class distribution')\n",
    "        plt.show()\n",
    "\n",
    "    return train_images_path, train_images_label, val_images_path, val_images_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " data_transform = {\n",
    "        \"train\": transforms.Compose([transforms.RandomResizedCrop(img_size[num_model][0]),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])]),\n",
    "        \"val\": transforms.Compose([transforms.Resize(img_size[num_model][1]),\n",
    "                                   transforms.CenterCrop(img_size[num_model][1]),\n",
    "                                   transforms.ToTensor(),\n",
    "                                   transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:cv]",
   "language": "python",
   "name": "conda-env-cv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
